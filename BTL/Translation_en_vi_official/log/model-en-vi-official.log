15.7s 1 Collecting sacrebleu
15.8s 2 Downloading sacrebleu-2.4.2-py3-none-any.whl.metadata (58 kB)
15.8s 3 [?25l     [90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m [32m0.0/58.0 kB[0m [31m?[0m eta [36m-:--:--[0m[2K     [91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[91m‚ï∏[0m[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m [32m41.0/58.0 kB[0m [31m999.0 kB/s[0m eta [36m0:00:01[0m[2K     [90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m [32m58.0/58.0 kB[0m [31m935.2 kB/s[0m eta [36m0:00:00[0m
15.9s 4 [?25hCollecting portalocker (from sacrebleu)
16.0s 5 Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)
16.0s 6 Requirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (2023.12.25)
16.0s 7 Requirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.9.0)
16.0s 8 Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (1.26.4)
16.0s 9 Requirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.4.6)
16.0s 10 Requirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (5.2.2)
16.1s 11 Downloading sacrebleu-2.4.2-py3-none-any.whl (106 kB)
16.1s 12 [?25l   [90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m [32m0.0/106.7 kB[0m [31m?[0m eta [36m-:--:--[0m[2K   [91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[91m‚ï∏[0m[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m [32m81.9/106.7 kB[0m [31m2.3 MB/s[0m eta [36m0:00:01[0m[2K   [90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m [32m106.7/106.7 kB[0m [31m1.6 MB/s[0m eta [36m0:00:00[0m
16.2s 13 [?25hDownloading portalocker-2.10.1-py3-none-any.whl (18 kB)
28.1s 14 Installing collected packages: portalocker, sacrebleu
28.3s 15 Successfully installed portalocker-2.10.1 sacrebleu-2.4.2
29.6s 16 Collecting rouge-score
29.7s 17 Downloading rouge_score-0.1.2.tar.gz (17 kB)
31.0s 18 Preparing metadata (setup.py) ... [?25l- done
31.0s 19 [?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.4.0)
31.0s 20 Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge-score) (3.2.4)
31.0s 21 Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.26.4)
31.0s 22 Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.16.0)
31.1s 23 Building wheels for collected packages: rouge-score
32.6s 24 Building wheel for rouge-score (setup.py) ... [?25l- \ done
32.6s 25 [?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=625525a4adc177f5a70448e898d65cb1b0a93d0349fa83637d1b06e4bed5c489
32.6s 26 Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4
32.6s 27 Successfully built rouge-score
42.6s 28 Installing collected packages: rouge-score
42.8s 29 Successfully installed rouge-score-0.1.2
43.2s 30 Note: you may need to restart the kernel to use updated packages.
56.9s 31 2024-08-04 08:17:09.629217: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
56.9s 32 2024-08-04 08:17:09.629369: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
56.9s 33 2024-08-04 08:17:09.903206: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
57.1s 34 2024-08-04 08:17:09.629217: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
57.1s 35 2024-08-04 08:17:09.629369: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
57.1s 36 2024-08-04 08:17:09.903206: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
80.4s 37 /opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
80.4s 38 return self.fget.__get__(instance, owner)()
83.6s 39 /opt/conda/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.
83.6s 40 warnings.warn("Recommended: pip install sacremoses.")
85.3s 41 /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4016: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
85.3s 42 warnings.warn(
144.2s 43 [34m[1mwandb[0m: W&B API key is configured. Use [1m`wandb login --relogin`[0m to force relogin
144.2s 44 [34m[1mwandb[0m: [33mWARNING[0m If you're specifying your api key in code, ensure this code is not shared publicly.
144.2s 45 [34m[1mwandb[0m: [33mWARNING[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.
144.2s 46 [34m[1mwandb[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc
144.4s 47 /opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
144.4s 48 warnings.warn(
145.9s 49 [34m[1mwandb[0m: [33mWARNING[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
145.9s 50 [34m[1mwandb[0m: Currently logged in as: [33mphuctranmtms1[0m ([33mphuctranmtms1-mtms1[0m). Use [1m`wandb login --relogin`[0m to force relogin
161.2s 51 [34m[1mwandb[0m: wandb version 0.17.5 is available!  To upgrade, please run:
161.2s 52 [34m[1mwandb[0m:  $ pip install wandb --upgrade
161.3s 53 [34m[1mwandb[0m: Tracking run with wandb version 0.17.4
161.3s 54 [34m[1mwandb[0m: Run data is saved locally in [35m[1m/kaggle/working/wandb/run-20240804_081839-fzh1co5a[0m
161.3s 55 [34m[1mwandb[0m: Run [1m`wandb offline`[0m to turn off syncing.
161.3s 56 [34m[1mwandb[0m: Syncing run [33m./results-en-vi[0m
161.3s 57 [34m[1mwandb[0m: ‚≠êÔ∏è View project at [34m[4mhttps://wandb.ai/phuctranmtms1-mtms1/huggingface[0m
161.3s 58 [34m[1mwandb[0m: üöÄ View run at [34m[4mhttps://wandb.ai/phuctranmtms1-mtms1/huggingface/runs/fzh1co5a[0m
164.3s 59 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
164.3s 60 warnings.warn('Was asked to gather along dimension 0, but all '
549.5s 61 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
549.5s 62 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
551.6s 63 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
551.6s 64 warnings.warn('Was asked to gather along dimension 0, but all '
850.9s 65 Translated: Ch√∫ng t√¥i ƒë·ªÅu l√† nh·ªØng b√°c sƒ© gi·ªèi, v√† anh ·∫•y c≈©ng v·∫≠y. V·∫≠y ca ph·∫´u thu·∫≠t ch·∫Øc ch·∫Øn s·∫Ω th√†nh c√¥ng.
942.2s 66 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
942.2s 67 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
944.0s 68 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
944.0s 69 warnings.warn('Was asked to gather along dimension 0, but all '
1334.2s 70 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
1334.2s 71 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
1336.1s 72 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
1336.1s 73 warnings.warn('Was asked to gather along dimension 0, but all '
1636.3s 74 Translated: C·∫£ hai ch√∫ng t√¥i ƒë·ªÅu l√† nh·ªØng b√°c sƒ© xu·∫•t s·∫Øc, v√† anh ·∫•y c≈©ng v·∫≠y. Ca ph·∫´u thu·∫≠t ch·∫Øc ch·∫Øn s·∫Ω th√†nh c√¥ng.
1729.1s 75 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
1729.1s 76 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
1731.0s 77 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
1731.0s 78 warnings.warn('Was asked to gather along dimension 0, but all '
2121.3s 79 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
2121.3s 80 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
2123.2s 81 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
2123.2s 82 warnings.warn('Was asked to gather along dimension 0, but all '
2423.8s 83 Translated: Ch√∫ng ta ƒë·ªÅu l√† nh·ªØng b√°c sƒ© gi·ªèi, v√† anh ta c≈©ng v·∫≠y. Ca ph·∫´u thu·∫≠t ch·∫Øc ch·∫Øn s·∫Ω th√†nh c√¥ng.
2515.7s 84 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
2515.7s 85 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
2517.7s 86 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
2517.7s 87 warnings.warn('Was asked to gather along dimension 0, but all '
2909.6s 88 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
2909.6s 89 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
2911.6s 90 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
2911.6s 91 warnings.warn('Was asked to gather along dimension 0, but all '
3211.5s 92 Translated: Ch√∫ng ta ƒë·ªÅu l√† nh·ªØng b√°c sƒ© gi·ªèi, v√† anh ta c≈©ng v·∫≠y. Ca ph·∫´u thu·∫≠t ch·∫Øc ch·∫Øn s·∫Ω th√†nh c√¥ng.
3303.4s 93 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
3303.4s 94 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
3305.4s 95 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
3305.4s 96 warnings.warn('Was asked to gather along dimension 0, but all '
3696.6s 97 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
3696.6s 98 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
3698.6s 99 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
3698.6s 100 warnings.warn('Was asked to gather along dimension 0, but all '
3998.2s 101 Translated: Ch√∫ng ta ƒë·ªÅu l√† nh·ªØng b√°c sƒ© gi·ªèi, v√† anh ta c≈©ng v·∫≠y. V·∫≠y n√™n ca ph·∫´u thu·∫≠t ch·∫Øc ch·∫Øn s·∫Ω th√†nh c√¥ng.
4089.6s 102 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
4089.6s 103 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
4091.5s 104 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
4091.5s 105 warnings.warn('Was asked to gather along dimension 0, but all '
4483.7s 106 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
4483.7s 107 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
4485.6s 108 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
4485.6s 109 warnings.warn('Was asked to gather along dimension 0, but all '
4786.3s 110 Translated: Ch√∫ng ta ƒë·ªÅu l√† nh·ªØng b√°c sƒ© gi·ªèi, v√† anh ta c≈©ng v·∫≠y. V·∫≠y n√™n ca ph·∫´u thu·∫≠t ch·∫Øc ch·∫Øn s·∫Ω th√†nh c√¥ng.
4879.5s 111 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
4879.5s 112 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
4881.4s 113 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
4881.4s 114 warnings.warn('Was asked to gather along dimension 0, but all '
5271.5s 115 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
5271.5s 116 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
5273.4s 117 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
5273.4s 118 warnings.warn('Was asked to gather along dimension 0, but all '
5573.1s 119 Translated: Ch√∫ng ta ƒë·ªÅu l√† nh·ªØng b√°c s·ªπ xu·∫•t s·∫Øc, v√† √¥ng ·∫•y c≈©ng v·∫≠y. V√¨ v·∫≠y, cu·ªôc ph·∫´u thu·∫≠t ch·∫Øc ch·∫Øn s·∫Ω th√†nh c√¥ng.
5665.4s 120 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
5665.4s 121 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
5667.3s 122 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
5667.3s 123 warnings.warn('Was asked to gather along dimension 0, but all '
6058.3s 124 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
6058.3s 125 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
6060.2s 126 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
6060.2s 127 warnings.warn('Was asked to gather along dimension 0, but all '
6360.9s 128 Translated: Ch√∫ng ta ƒë·ªÅu l√† nh·ªØng b√°c sƒ© xu·∫•t s·∫Øc, v√† anh ta c≈©ng v·∫≠y. V·∫≠y l√† ca ph·∫´u thu·∫≠t ch·∫Øc ch·∫Øn s·∫Ω th√†nh c√¥ng.
6453.4s 129 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
6453.4s 130 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
6455.4s 131 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
6455.4s 132 warnings.warn('Was asked to gather along dimension 0, but all '
6845.9s 133 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
6845.9s 134 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
6847.8s 135 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
6847.8s 136 warnings.warn('Was asked to gather along dimension 0, but all '
7148.5s 137 Translated: Ch√∫ng ta ƒë·ªÅu l√† nh·ªØng b√°c sƒ© gi·ªèi, v√† anh ·∫•y c≈©ng v·∫≠y. V·∫≠y n√™n cu·ªôc ph·∫´u thu·∫≠t ch·∫Øc ch·∫Øn s·∫Ω th√†nh c√¥ng.
7239.8s 138 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
7239.8s 139 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
7241.8s 140 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
7241.8s 141 warnings.warn('Was asked to gather along dimension 0, but all '
7635.5s 142 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
7635.5s 143 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
7637.5s 144 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
7637.5s 145 warnings.warn('Was asked to gather along dimension 0, but all '
7938.3s 146 Translated: C·∫£ hai ch√∫ng t√¥i ƒë·ªÅu l√† nh·ªØng b√°c sƒ© xu·∫•t s·∫Øc, v√† anh ·∫•y c≈©ng v·∫≠y. V√¨ v·∫≠y, cu·ªôc ph·∫´u thu·∫≠t ch·∫Øc ch·∫Øn s·∫Ω th√†nh c√¥ng.
8029.1s 147 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
8029.1s 148 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
8031.1s 149 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
8031.1s 150 warnings.warn('Was asked to gather along dimension 0, but all '
8424.2s 151 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
8424.2s 152 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
8426.1s 153 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
8426.1s 154 warnings.warn('Was asked to gather along dimension 0, but all '
8725.2s 155 Translated: C·∫£ hai ch√∫ng t√¥i ƒë·ªÅu l√† nh·ªØng b√°c sƒ© gi·ªèi, v√† anh ·∫•y c≈©ng v·∫≠y. V·∫≠y n√™n cu·ªôc ph·∫´u thu·∫≠t ch·∫Øc ch·∫Øn s·∫Ω th√†nh c√¥ng.
8816.1s 156 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
8816.1s 157 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
8818.0s 158 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
8818.0s 159 warnings.warn('Was asked to gather along dimension 0, but all '
9209.4s 160 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
9209.4s 161 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
9211.3s 162 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
9211.3s 163 warnings.warn('Was asked to gather along dimension 0, but all '
9511.8s 164 Translated: Ch√∫ng t√¥i ƒë·ªÅu l√† nh·ªØng b√°c sƒ© gi·ªèi, v√† anh ta c≈©ng th·∫ø. Ca ph·∫´u thu·∫≠t ch·∫Øc ch·∫Øn s·∫Ω th√†nh c√¥ng.
9604.4s 165 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
9604.4s 166 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
9606.3s 167 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
9606.3s 168 warnings.warn('Was asked to gather along dimension 0, but all '
9999.4s 169 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
9999.4s 170 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
10001.2s 171 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
10001.2s 172 warnings.warn('Was asked to gather along dimension 0, but all '
10301.8s 173 Translated: Ch√∫ng ta ƒë·ªÅu l√† nh·ªØng b√°c sƒ© xu·∫•t s·∫Øc, v√† anh ta c≈©ng v·∫≠y. V·∫≠y n√™n cu·ªôc ph·∫´u thu·∫≠t ch·∫Øc ch·∫Øn s·∫Ω th√†nh c√¥ng.
10394.9s 174 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
10394.9s 175 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
10396.9s 176 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
10396.9s 177 warnings.warn('Was asked to gather along dimension 0, but all '
10790.5s 178 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
10790.5s 179 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
10792.5s 180 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
10792.5s 181 warnings.warn('Was asked to gather along dimension 0, but all '
11092.2s 182 Translated: Ch√∫ng ta ƒë·ªÅu l√† nh·ªØng b√°c sƒ© xu·∫•t s·∫Øc, v√† anh ta c≈©ng v·∫≠y. V·∫≠y n√™n ca ph·∫´u thu·∫≠t ch·∫Øc ch·∫Øn s·∫Ω th√†nh c√¥ng.
11184.5s 183 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
11184.5s 184 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
11186.5s 185 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
11186.5s 186 warnings.warn('Was asked to gather along dimension 0, but all '
11578.1s 187 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
11578.1s 188 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
11580.1s 189 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
11580.1s 190 warnings.warn('Was asked to gather along dimension 0, but all '
11880.1s 191 Translated: C·∫£ hai ch√∫ng t√¥i ƒë·ªÅu l√† nh·ªØng b√°c sƒ© gi·ªèi, v√† anh ·∫•y c≈©ng v·∫≠y. V·∫≠y n√™n cu·ªôc ph·∫´u thu·∫≠t ch·∫Øc ch·∫Øn s·∫Ω th√†nh c√¥ng.
11973.3s 192 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
11973.3s 193 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
11975.3s 194 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
11975.3s 195 warnings.warn('Was asked to gather along dimension 0, but all '
12367.8s 196 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
12367.8s 197 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
12369.8s 198 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
12369.8s 199 warnings.warn('Was asked to gather along dimension 0, but all '
12670.1s 200 Translated: Ch√∫ng ta ƒë·ªÅu l√† nh·ªØng b√°c sƒ© gi·ªèi, v√† anh ta c≈©ng v·∫≠y. V√¨ v·∫≠y ca ph·∫´u thu·∫≠t ch·∫Øc ch·∫Øn s·∫Ω th√†nh c√¥ng.
12762.6s 201 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
12762.6s 202 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
12764.6s 203 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
12764.6s 204 warnings.warn('Was asked to gather along dimension 0, but all '
13160.1s 205 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
13160.1s 206 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
13162.1s 207 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
13162.1s 208 warnings.warn('Was asked to gather along dimension 0, but all '
13463.5s 209 Translated: C·∫£ hai ch√∫ng t√¥i ƒë·ªÅu l√† nh·ªØng b√°c sƒ© xu·∫•t s·∫Øc, v√† anh ·∫•y c≈©ng v·∫≠y. Ca ph·∫´u thu·∫≠t ch·∫Øc ch·∫Øn s·∫Ω th√†nh c√¥ng.
13556.7s 210 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
13556.7s 211 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
13558.6s 212 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
13558.6s 213 warnings.warn('Was asked to gather along dimension 0, but all '
13951.2s 214 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
13951.2s 215 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
13953.1s 216 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
13953.1s 217 warnings.warn('Was asked to gather along dimension 0, but all '
14253.3s 218 Translated: Ch√∫ng ta ƒë·ªÅu l√† nh·ªØng b√°c sƒ© xu·∫•t s·∫Øc, v√† anh ·∫•y c≈©ng v·∫≠y. V√¨ v·∫≠y, ca ph·∫´u thu·∫≠t ch·∫Øc ch·∫Øn s·∫Ω th√†nh c√¥ng.
14345.8s 219 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
14345.8s 220 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
14347.7s 221 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
14347.7s 222 warnings.warn('Was asked to gather along dimension 0, but all '
14740.1s 223 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
14740.1s 224 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
14742.1s 225 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
14742.1s 226 warnings.warn('Was asked to gather along dimension 0, but all '
15042.5s 227 Translated: C·∫£ hai ch√∫ng t√¥i ƒë·ªÅu l√† nh·ªØng b√°c sƒ© tuy·ªát v·ªùi, v√† anh ta c≈©ng v·∫≠y. Ca ph·∫´u thu·∫≠t ch·∫Øc ch·∫Øn s·∫Ω th√†nh c√¥ng.
15134.5s 228 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
15134.5s 229 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
15136.4s 230 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
15136.4s 231 warnings.warn('Was asked to gather along dimension 0, but all '
15527.7s 232 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
15527.7s 233 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
15529.6s 234 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
15529.6s 235 warnings.warn('Was asked to gather along dimension 0, but all '
15831.6s 236 Translated: Ch√∫ng ta ƒë·ªÅu l√† nh·ªØng b√°c sƒ© xu·∫•t s·∫Øc, v√† anh ta c≈©ng v·∫≠y. V√¨ v·∫≠y, ca ph·∫´u thu·∫≠t ch·∫Øc ch·∫Øn s·∫Ω th√†nh c√¥ng.
15925.4s 237 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
15925.4s 238 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
15927.3s 239 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
15927.3s 240 warnings.warn('Was asked to gather along dimension 0, but all '
16317.7s 241 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
16317.7s 242 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
16319.6s 243 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
16319.6s 244 warnings.warn('Was asked to gather along dimension 0, but all '
16621.1s 245 Translated: C·∫£ hai ch√∫ng t√¥i ƒë·ªÅu l√† nh·ªØng b√°c sƒ© xu·∫•t s·∫Øc, v√† anh ta c≈©ng v·∫≠y. V·∫≠y n√™n ca ph·∫´u thu·∫≠t ch·∫Øc ch·∫Øn s·∫Ω th√†nh c√¥ng.
16713.9s 246 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
16713.9s 247 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
16715.9s 248 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
16715.9s 249 warnings.warn('Was asked to gather along dimension 0, but all '
17109.4s 250 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
17109.4s 251 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
17111.4s 252 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
17111.4s 253 warnings.warn('Was asked to gather along dimension 0, but all '
17412.3s 254 Translated: Ch√∫ng ta ƒë·ªÅu l√† nh·ªØng b√°c sƒ© gi·ªèi, v√† anh ta c≈©ng v·∫≠y. V·∫≠y n√™n cu·ªôc ph·∫´u thu·∫≠t ch·∫Øc ch·∫Øn s·∫Ω th√†nh c√¥ng.
17504.6s 255 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
17504.6s 256 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
17506.5s 257 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
17506.5s 258 warnings.warn('Was asked to gather along dimension 0, but all '
17900.8s 259 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
17900.8s 260 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
17902.8s 261 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
17902.8s 262 warnings.warn('Was asked to gather along dimension 0, but all '
18203.2s 263 Translated: Ch√∫ng t√¥i ƒë·ªÅu l√† nh·ªØng b√°c sƒ© xu·∫•t s·∫Øc, v√† anh ta c≈©ng v·∫≠y. V√¨ v·∫≠y, cu·ªôc ph·∫´u thu·∫≠t ch·∫Øc ch·∫Øn s·∫Ω th√†nh c√¥ng.
18296.4s 264 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
18296.4s 265 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
18298.3s 266 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
18298.3s 267 warnings.warn('Was asked to gather along dimension 0, but all '
18692.7s 268 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
18692.7s 269 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
18694.7s 270 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
18694.7s 271 warnings.warn('Was asked to gather along dimension 0, but all '
18996.1s 272 Translated: C·∫£ hai ch√∫ng t√¥i ƒë·ªÅu l√† nh·ªØng b√°c sƒ© tuy·ªát v·ªùi, v√† anh ·∫•y c≈©ng v·∫≠y. V·∫≠y n√™n ca ph·∫´u thu·∫≠t ch·∫Øc ch·∫Øn s·∫Ω th√†nh c√¥ng.
19089.3s 273 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
19089.3s 274 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
19091.4s 275 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
19091.4s 276 warnings.warn('Was asked to gather along dimension 0, but all '
19485.5s 277 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
19485.5s 278 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
19487.4s 279 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
19487.4s 280 warnings.warn('Was asked to gather along dimension 0, but all '
19788.3s 281 Translated: Ch√∫ng t√¥i ƒë·ªÅu l√† nh·ªØng b√°c sƒ© gi·ªèi, v√† anh ·∫•y c≈©ng v·∫≠y. V√¨ v·∫≠y ca ph·∫´u thu·∫≠t ch·∫Øc ch·∫Øn s·∫Ω th√†nh c√¥ng.
19880.4s 282 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
19880.4s 283 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
19882.4s 284 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
19882.4s 285 warnings.warn('Was asked to gather along dimension 0, but all '
20276.3s 286 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
20276.3s 287 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
20278.3s 288 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
20278.3s 289 warnings.warn('Was asked to gather along dimension 0, but all '
20578.1s 290 Translated: C·∫£ hai ch√∫ng t√¥i ƒë·ªÅu l√† nh·ªØng b√°c sƒ© gi·ªèi, v√† anh ta c≈©ng v·∫≠y. V·∫≠y n√™n ca ph·∫´u thu·∫≠t ch·∫Øc ch·∫Øn s·∫Ω th√†nh c√¥ng.
20670.4s 291 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
20670.4s 292 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
20672.3s 293 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
20672.3s 294 warnings.warn('Was asked to gather along dimension 0, but all '
21066.0s 295 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
21066.0s 296 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
21068.0s 297 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
21068.0s 298 warnings.warn('Was asked to gather along dimension 0, but all '
21369.2s 299 Translated: Ch√∫ng t√¥i ƒë·ªÅu l√† nh·ªØng b√°c sƒ© gi·ªèi, v√† anh ta c≈©ng v·∫≠y. V·∫≠y n√™n ca ph·∫´u thu·∫≠t ch·∫Øc ch·∫Øn s·∫Ω th√†nh c√¥ng.
21463.2s 300 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
21463.2s 301 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
21465.1s 302 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
21465.1s 303 warnings.warn('Was asked to gather along dimension 0, but all '
21859.4s 304 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
21859.4s 305 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
21861.4s 306 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
21861.4s 307 warnings.warn('Was asked to gather along dimension 0, but all '
22162.1s 308 Translated: Ch√∫ng t√¥i ƒë·ªÅu l√† nh·ªØng b√°c sƒ© gi·ªèi, v√† anh ·∫•y c≈©ng v·∫≠y. V·∫≠y n√™n ca ph·∫´u thu·∫≠t ch·∫Øc ch·∫Øn s·∫Ω th√†nh c√¥ng.
22255.3s 309 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
22255.3s 310 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
22257.3s 311 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
22257.3s 312 warnings.warn('Was asked to gather along dimension 0, but all '
22649.9s 313 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
22649.9s 314 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
22651.8s 315 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
22651.8s 316 warnings.warn('Was asked to gather along dimension 0, but all '
22952.7s 317 Translated: C·∫£ hai ch√∫ng t√¥i ƒë·ªÅu l√† nh·ªØng b√°c sƒ© gi·ªèi, v√† anh ·∫•y c≈©ng v·∫≠y. Ca ph·∫´u thu·∫≠t ch·∫Øc ch·∫Øn s·∫Ω th√†nh c√¥ng.
23046.6s 318 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
23046.6s 319 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
23048.5s 320 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
23048.5s 321 warnings.warn('Was asked to gather along dimension 0, but all '
23440.4s 322 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
23440.4s 323 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
23442.4s 324 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
23442.4s 325 warnings.warn('Was asked to gather along dimension 0, but all '
23742.7s 326 Translated: C·∫£ hai ch√∫ng t√¥i ƒë·ªÅu l√† nh·ªØng b√°c sƒ© xu·∫•t s·∫Øc, v√† anh ·∫•y c≈©ng v·∫≠y. V·∫≠y n√™n ca ph·∫´u thu·∫≠t ch·∫Øc ch·∫Øn s·∫Ω th√†nh c√¥ng.
23835.5s 327 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
23835.5s 328 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
23837.4s 329 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
23837.4s 330 warnings.warn('Was asked to gather along dimension 0, but all '
24230.6s 331 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
24230.6s 332 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
24232.6s 333 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
24232.6s 334 warnings.warn('Was asked to gather along dimension 0, but all '
24533.9s 335 Translated: Ch√∫ng t√¥i ƒë·ªÅu l√† nh·ªØng b√°c sƒ© gi·ªèi, v√† anh ·∫•y c≈©ng v·∫≠y. V·∫≠y n√™n ca ph·∫´u thu·∫≠t ch·∫Øc ch·∫Øn s·∫Ω th√†nh c√¥ng.
24627.1s 336 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
24627.1s 337 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
24629.0s 338 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
24629.0s 339 warnings.warn('Was asked to gather along dimension 0, but all '
25022.9s 340 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
25022.9s 341 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
25024.8s 342 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
25024.8s 343 warnings.warn('Was asked to gather along dimension 0, but all '
25325.6s 344 Translated: C·∫£ hai ch√∫ng t√¥i ƒë·ªÅu l√† nh·ªØng b√°c sƒ© gi·ªèi, v√† anh ·∫•y c≈©ng v·∫≠y. V√¨ v·∫≠y, cu·ªôc ph·∫´u thu·∫≠t ch·∫Øc ch·∫Øn s·∫Ω th√†nh c√¥ng.
25418.9s 345 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
25418.9s 346 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
25420.8s 347 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
25420.8s 348 warnings.warn('Was asked to gather along dimension 0, but all '
25815.2s 349 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
25815.2s 350 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
25817.5s 351 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
25817.5s 352 warnings.warn('Was asked to gather along dimension 0, but all '
26117.5s 353 Translated: Ch√∫ng t√¥i ƒë·ªÅu l√† nh·ªØng b√°c sƒ© xu·∫•t s·∫Øc, anh ·∫•y c≈©ng v·∫≠y. Ca ph·∫´u thu·∫≠t ch·∫Øc ch·∫Øn s·∫Ω th√†nh c√¥ng.
26209.8s 354 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
26209.8s 355 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
26212.1s 356 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
26212.1s 357 warnings.warn('Was asked to gather along dimension 0, but all '
26607.2s 358 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
26607.2s 359 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
26609.2s 360 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
26609.2s 361 warnings.warn('Was asked to gather along dimension 0, but all '
26910.4s 362 Translated: Ch√∫ng t√¥i ƒë·ªÅu l√† nh·ªØng b√°c sƒ© xu·∫•t s·∫Øc, v√† anh ·∫•y c≈©ng v·∫≠y. V·∫≠y n√™n ca ph·∫´u thu·∫≠t ch·∫Øc ch·∫Øn s·∫Ω th√†nh c√¥ng.
27003.8s 363 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
27003.8s 364 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
27005.7s 365 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
27005.7s 366 warnings.warn('Was asked to gather along dimension 0, but all '
27397.7s 367 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
27397.7s 368 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
27399.6s 369 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
27399.6s 370 warnings.warn('Was asked to gather along dimension 0, but all '
27700.6s 371 Translated: Ch√∫ng t√¥i ƒë·ªÅu l√† nh·ªØng b√°c sƒ© tuy·ªát v·ªùi, v√† anh ·∫•y c≈©ng v·∫≠y. V√¨ v·∫≠y, cu·ªôc ph·∫´u thu·∫≠t ch·∫Øc ch·∫Øn s·∫Ω th√†nh c√¥ng.
27793.0s 372 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
27793.0s 373 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
27794.9s 374 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
27794.9s 375 warnings.warn('Was asked to gather along dimension 0, but all '
28188.7s 376 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
28188.7s 377 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
28190.7s 378 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
28190.7s 379 warnings.warn('Was asked to gather along dimension 0, but all '
28491.6s 380 Translated: Ch√∫ng t√¥i ƒë·ªÅu l√† nh·ªØng b√°c sƒ© xu·∫•t s·∫Øc, v√† anh ·∫•y c≈©ng v·∫≠y. Ca ph·∫´u thu·∫≠t ch·∫Øc ch·∫Øn s·∫Ω th√†nh c√¥ng.
28584.5s 381 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
28584.5s 382 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
28586.4s 383 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
28586.4s 384 warnings.warn('Was asked to gather along dimension 0, but all '
28980.7s 385 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
28980.7s 386 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
28982.6s 387 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
28982.6s 388 warnings.warn('Was asked to gather along dimension 0, but all '
29283.8s 389 Translated: C·∫£ hai ch√∫ng t√¥i ƒë·ªÅu l√† nh·ªØng b√°c sƒ© gi·ªèi, v√† anh ta c≈©ng v·∫≠y. V√¨ v·∫≠y, cu·ªôc ph·∫´u thu·∫≠t ch·∫Øc ch·∫Øn s·∫Ω th√†nh c√¥ng.
29377.9s 390 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
29377.9s 391 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
29379.9s 392 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
29379.9s 393 warnings.warn('Was asked to gather along dimension 0, but all '
29774.1s 394 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
29774.1s 395 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
29776.0s 396 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
29776.0s 397 warnings.warn('Was asked to gather along dimension 0, but all '
30076.9s 398 Translated: Ch√∫ng ta ƒë·ªÅu l√† nh·ªØng b√°c sƒ© xu·∫•t s·∫Øc, v√† anh ta c≈©ng v·∫≠y. V√¨ v·∫≠y, ca ph·∫´u thu·∫≠t ch·∫Øc ch·∫Øn s·∫Ω th√†nh c√¥ng.
30170.3s 399 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
30170.3s 400 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
30172.2s 401 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
30172.2s 402 warnings.warn('Was asked to gather along dimension 0, but all '
30565.8s 403 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
30565.8s 404 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
30567.8s 405 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
30567.8s 406 warnings.warn('Was asked to gather along dimension 0, but all '
30868.4s 407 Translated: C·∫£ hai ch√∫ng t√¥i ƒë·ªÅu l√† nh·ªØng b√°c sƒ© gi·ªèi, v√† anh ·∫•y c≈©ng v·∫≠y. V·∫≠y n√™n ca ph·∫´u thu·∫≠t ch·∫Øc ch·∫Øn s·∫Ω th√†nh c√¥ng.
30961.2s 408 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
30961.2s 409 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
30963.1s 410 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
30963.1s 411 warnings.warn('Was asked to gather along dimension 0, but all '
31356.3s 412 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
31356.3s 413 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
31358.2s 414 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
31358.2s 415 warnings.warn('Was asked to gather along dimension 0, but all '
31658.7s 416 Translated: Ch√∫ng t√¥i ƒë·ªÅu l√† nh·ªØng b√°c sƒ© gi·ªèi, v√† anh ·∫•y c≈©ng v·∫≠y. V√¨ v·∫≠y, ca ph·∫´u thu·∫≠t ch·∫Øc ch·∫Øn s·∫Ω th√†nh c√¥ng.
31751.3s 417 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
31751.3s 418 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
31753.3s 419 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
31753.3s 420 warnings.warn('Was asked to gather along dimension 0, but all '
32147.3s 421 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
32147.3s 422 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
32149.2s 423 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
32149.2s 424 warnings.warn('Was asked to gather along dimension 0, but all '
32449.6s 425 Translated: Ch√∫ng ta ƒë·ªÅu l√† nh·ªØng b√°c sƒ© gi·ªèi, v√† anh ta c≈©ng v·∫≠y. V·∫≠y n√™n ca ph·∫´u thu·∫≠t ch·∫Øc ch·∫Øn s·∫Ω th√†nh c√¥ng.
32542.0s 426 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
32542.0s 427 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
32544.0s 428 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
32544.0s 429 warnings.warn('Was asked to gather along dimension 0, but all '
32937.5s 430 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
32937.5s 431 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
32939.4s 432 /opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
32939.4s 433 warnings.warn('Was asked to gather along dimension 0, but all '
33009.9s 434 Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
33009.9s 435 Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[53684]], 'forced_eos_token_id': 0}
33016.3s 436 Traceback (most recent call last):
33016.3s 437 File "<string>", line 1, in <module>
33016.3s 438 File "/opt/conda/lib/python3.10/site-packages/papermill/execute.py", line 134, in execute_notebook
33016.3s 439 raise_for_execution_errors(nb, output_path)
33016.3s 440 File "/opt/conda/lib/python3.10/site-packages/papermill/execute.py", line 241, in raise_for_execution_errors
33016.3s 441 raise error
33016.3s 442 papermill.exceptions.PapermillExecutionError:
33016.3s 443 ---------------------------------------------------------------------------
33016.3s 444 Exception encountered at "In [14]":
33016.3s 445 [0;31m---------------------------------------------------------------------------[0m
33016.3s 446 [0;31mFileNotFoundError[0m                         Traceback (most recent call last)
33016.3s 447 Cell [0;32mIn[14], line 1[0m
33016.3s 448 [0;32m----> 1[0m state_dict [38;5;241m=[39m [43mtorch[49m[38;5;241;43m.[39;49m[43mload[49m[43m([49m[38;5;124;43m'[39;49m[38;5;124;43m/kaggle/working/T5-en-vi-parameters.pth[39;49m[38;5;124;43m'[39;49m[43m)[49m
33016.3s 449 [1;32m      3[0m [38;5;66;03m# C·∫≠p nh·∫≠t tr·ªçng s·ªë v√†o m√¥ h√¨nh[39;00m
33016.3s 450 [1;32m      4[0m model_T5_en_vi[38;5;241m.[39mload_state_dict(state_dict)
33016.3s 451 
33016.3s 452 File [0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:986[0m, in [0;36mload[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)[0m
33016.3s 453 [1;32m    983[0m [38;5;28;01mif[39;00m [38;5;124m'[39m[38;5;124mencoding[39m[38;5;124m'[39m [38;5;129;01mnot[39;00m [38;5;129;01min[39;00m pickle_load_args[38;5;241m.[39mkeys():
33016.3s 454 [1;32m    984[0m     pickle_load_args[[38;5;124m'[39m[38;5;124mencoding[39m[38;5;124m'[39m] [38;5;241m=[39m [38;5;124m'[39m[38;5;124mutf-8[39m[38;5;124m'[39m
33016.3s 455 [0;32m--> 986[0m [38;5;28;01mwith[39;00m [43m_open_file_like[49m[43m([49m[43mf[49m[43m,[49m[43m [49m[38;5;124;43m'[39;49m[38;5;124;43mrb[39;49m[38;5;124;43m'[39;49m[43m)[49m [38;5;28;01mas[39;00m opened_file:
33016.3s 456 [1;32m    987[0m     [38;5;28;01mif[39;00m _is_zipfile(opened_file):
33016.3s 457 [1;32m    988[0m         [38;5;66;03m# The zipfile reader is going to advance the current file position.[39;00m
33016.3s 458 [1;32m    989[0m         [38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to[39;00m
33016.3s 459 [1;32m    990[0m         [38;5;66;03m# reset back to the original position.[39;00m
33016.3s 460 [1;32m    991[0m         orig_position [38;5;241m=[39m opened_file[38;5;241m.[39mtell()
33016.3s 461 
33016.3s 462 File [0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:435[0m, in [0;36m_open_file_like[0;34m(name_or_buffer, mode)[0m
33016.3s 463 [1;32m    433[0m [38;5;28;01mdef[39;00m [38;5;21m_open_file_like[39m(name_or_buffer, mode):
33016.3s 464 [1;32m    434[0m     [38;5;28;01mif[39;00m _is_path(name_or_buffer):
33016.3s 465 [0;32m--> 435[0m         [38;5;28;01mreturn[39;00m [43m_open_file[49m[43m([49m[43mname_or_buffer[49m[43m,[49m[43m [49m[43mmode[49m[43m)[49m
33016.3s 466 [1;32m    436[0m     [38;5;28;01melse[39;00m:
33016.3s 467 [1;32m    437[0m         [38;5;28;01mif[39;00m [38;5;124m'[39m[38;5;124mw[39m[38;5;124m'[39m [38;5;129;01min[39;00m mode:
33016.3s 468 
33016.3s 469 File [0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:416[0m, in [0;36m_open_file.__init__[0;34m(self, name, mode)[0m
33016.3s 470 [1;32m    415[0m [38;5;28;01mdef[39;00m [38;5;21m__init__[39m([38;5;28mself[39m, name, mode):
33016.3s 471 [0;32m--> 416[0m     [38;5;28msuper[39m()[38;5;241m.[39m[38;5;21m__init__[39m([38;5;28;43mopen[39;49m[43m([49m[43mname[49m[43m,[49m[43m [49m[43mmode[49m[43m)[49m)
33016.3s 472 
33016.3s 473 [0;31mFileNotFoundError[0m: [Errno 2] No such file or directory: '/kaggle/working/T5-en-vi-parameters.pth'
33016.3s 474 
33018.6s 475 /opt/conda/lib/python3.10/site-packages/traitlets/traitlets.py:2930: FutureWarning: --Exporter.preprocessors=["remove_papermill_header.RemovePapermillHeader"] for containers is deprecated in traitlets 5.0. You can pass `--Exporter.preprocessors item` ... multiple times to add items to a list.
33018.6s 476 warn(
33018.7s 477 [NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
33018.7s 478 [NbConvertApp] Converting notebook __notebook__.ipynb to notebook
33019.2s 479 [NbConvertApp] Writing 254414 bytes to __notebook__.ipynb
33020.9s 480 /opt/conda/lib/python3.10/site-packages/traitlets/traitlets.py:2930: FutureWarning: --Exporter.preprocessors=["nbconvert.preprocessors.ExtractOutputPreprocessor"] for containers is deprecated in traitlets 5.0. You can pass `--Exporter.preprocessors item` ... multiple times to add items to a list.
33020.9s 481 warn(
33020.9s 482 [NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
33021.0s 483 [NbConvertApp] Converting notebook __notebook__.ipynb to html
33022.1s 484 [NbConvertApp] Writing 511436 bytes to __results__.html