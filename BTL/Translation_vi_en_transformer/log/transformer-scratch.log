9.6s 1 Collecting sacrebleu
9.6s 2 Downloading sacrebleu-2.4.2-py3-none-any.whl.metadata (58 kB)
9.7s 3 [?25l     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m0.0/58.0 kB[0m [31m?[0m eta [36m-:--:--[0m[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m58.0/58.0 kB[0m [31m1.7 MB/s[0m eta [36m0:00:00[0m
9.8s 4 [?25hCollecting portalocker (from sacrebleu)
9.8s 5 Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)
9.9s 6 Requirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (2023.12.25)
9.9s 7 Requirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.9.0)
9.9s 8 Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (1.26.4)
9.9s 9 Requirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.4.6)
9.9s 10 Requirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (5.2.2)
10.0s 11 Downloading sacrebleu-2.4.2-py3-none-any.whl (106 kB)
10.0s 12 [?25l   [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m0.0/106.7 kB[0m [31m?[0m eta [36m-:--:--[0m[2K   [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m106.7/106.7 kB[0m [31m3.9 MB/s[0m eta [36m0:00:00[0m
10.0s 13 [?25hDownloading portalocker-2.10.1-py3-none-any.whl (18 kB)
22.1s 14 Installing collected packages: portalocker, sacrebleu
22.5s 15 Successfully installed portalocker-2.10.1 sacrebleu-2.4.2
24.1s 16 Collecting rouge_score
24.1s 17 Downloading rouge_score-0.1.2.tar.gz (17 kB)
25.3s 18 Preparing metadata (setup.py) ... [?25l- done
25.3s 19 [?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)
25.3s 20 Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)
25.3s 21 Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.26.4)
25.3s 22 Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)
25.3s 23 Building wheels for collected packages: rouge_score
26.9s 24 Building wheel for rouge_score (setup.py) ... [?25l- \ done
26.9s 25 [?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=a6fa2980e9b94fc96e3d89d0a6e470ada5be1c10a110c988c1c09fa2cd22a50a
26.9s 26 Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4
26.9s 27 Successfully built rouge_score
38.3s 28 Installing collected packages: rouge_score
38.6s 29 Successfully installed rouge_score-0.1.2
40.1s 30 Collecting underthesea
40.2s 31 Downloading underthesea-6.8.4-py3-none-any.whl.metadata (15 kB)
40.3s 32 Requirement already satisfied: Click>=6.0 in /opt/conda/lib/python3.10/site-packages (from underthesea) (8.1.7)
40.4s 33 Collecting python-crfsuite>=0.9.6 (from underthesea)
40.4s 34 Downloading python_crfsuite-0.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)
40.4s 35 Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from underthesea) (3.2.4)
40.4s 36 Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from underthesea) (4.66.4)
40.4s 37 Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from underthesea) (2.32.3)
40.4s 38 Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from underthesea) (1.4.2)
40.4s 39 Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from underthesea) (1.2.2)
40.4s 40 Requirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from underthesea) (6.0.1)
40.5s 41 Collecting underthesea-core==1.0.4 (from underthesea)
40.5s 42 Downloading underthesea_core-1.0.4-cp310-cp310-manylinux2010_x86_64.whl.metadata (1.7 kB)
40.5s 43 Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk->underthesea) (1.16.0)
40.6s 44 Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->underthesea) (3.3.2)
40.6s 45 Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->underthesea) (3.6)
40.6s 46 Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->underthesea) (1.26.18)
40.6s 47 Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->underthesea) (2024.7.4)
40.6s 48 Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->underthesea) (1.26.4)
40.6s 49 Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->underthesea) (1.11.4)
40.6s 50 Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->underthesea) (3.2.0)
40.7s 51 Downloading underthesea-6.8.4-py3-none-any.whl (20.9 MB)
41.1s 52 [?25l   [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m0.0/20.9 MB[0m [31m?[0m eta [36m-:--:--[0m[2K   [91mâ•¸[0m[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m0.3/20.9 MB[0m [31m10.4 MB/s[0m eta [36m0:00:02[0m[2K   [91mâ”â”â”â”â”[0m[90mâ•º[0m[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m2.7/20.9 MB[0m [31m38.3 MB/s[0m eta [36m0:00:01[0m[2K   [91mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[90mâ•º[0m[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m7.0/20.9 MB[0m [31m66.4 MB/s[0m eta [36m0:00:01[0m[2K   [91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[90mâ•º[0m[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m11.8/20.9 MB[0m [31m123.5 MB/s[0m eta [36m0:00:01[0m[2K   [91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[91mâ•¸[0m[90mâ”â”â”â”â”â”â”â”[0m [32m16.6/20.9 MB[0m [31m133.7 MB/s[0m eta [36m0:00:01[0m[2K   [91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[91mâ•¸[0m [32m20.9/20.9 MB[0m [31m138.5 MB/s[0m eta [36m0:00:01[0m[2K   [91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[91mâ•¸[0m [32m20.9/20.9 MB[0m [31m138.5 MB/s[0m eta [36m0:00:01[0m[2K   [91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[91mâ•¸[0m [32m20.9/20.9 MB[0m [31m138.5 MB/s[0m eta [36m0:00:01[0m[2K   [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m20.9/20.9 MB[0m [31m62.6 MB/s[0m eta [36m0:00:00[0m
41.1s 53 [?25hDownloading underthesea_core-1.0.4-cp310-cp310-manylinux2010_x86_64.whl (657 kB)
41.1s 54 [?25l   [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m0.0/657.8 kB[0m [31m?[0m eta [36m-:--:--[0m[2K   [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m657.8/657.8 kB[0m [31m29.9 MB/s[0m eta [36m0:00:00[0m
41.2s 55 [?25hDownloading python_crfsuite-0.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)
41.2s 56 [?25l   [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m0.0/1.1 MB[0m [31m?[0m eta [36m-:--:--[0m[2K   [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m1.1/1.1 MB[0m [31m41.9 MB/s[0m eta [36m0:00:00[0m
52.6s 57 [?25hInstalling collected packages: underthesea-core, python-crfsuite, underthesea
53.5s 58 Successfully installed python-crfsuite-0.9.10 underthesea-6.8.4 underthesea-core-1.0.4
54.1s 59 Note: you may need to restart the kernel to use updated packages.
69.0s 60 /opt/conda/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
69.0s 61 warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
73.7s 62 /opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
73.7s 63 warnings.warn(
124.5s 64 Step: 21.322%, Train loss: 8.088, Val loss: 6.619, time: 0p51s
176.6s 65 Step: 42.644%, Train loss: 7.301, Val loss: 6.356, time: 0p52s
227.5s 66 Step: 63.966%, Train loss: 6.932, Val loss: 6.208, time: 0p50s
276.5s 67 Step: 85.288%, Train loss: 6.703, Val loss: 5.999, time: 0p48s
310.7s 68 And you you the the the .
310.7s 69 Epoch: 1, Train loss: 6.576, Val loss: 5.881, Epoch time: 237.856s
360.2s 70 Step: 21.322%, Train loss: 5.702, Val loss: 5.783, time: 0p49s
410.2s 71 Step: 42.644%, Train loss: 5.679, Val loss: 5.681, time: 0p50s
459.6s 72 Step: 63.966%, Train loss: 5.605, Val loss: 5.690, time: 0p49s
507.8s 73 Step: 85.288%, Train loss: 5.544, Val loss: 5.576, time: 0p48s
541.7s 74 And you ' t do .
541.7s 75 Epoch: 2, Train loss: 5.504, Val loss: 5.510, Epoch time: 230.954s
591.3s 76 Step: 21.322%, Train loss: 5.233, Val loss: 5.488, time: 0p49s
641.4s 77 Step: 42.644%, Train loss: 5.247, Val loss: 5.429, time: 0p50s
690.3s 78 Step: 63.966%, Train loss: 5.209, Val loss: 5.429, time: 0p48s
738.4s 79 Step: 85.288%, Train loss: 5.171, Val loss: 5.394, time: 0p48s
772.8s 80 And I ' t know .
772.8s 81 Epoch: 3, Train loss: 5.150, Val loss: 5.377, Epoch time: 231.070s
823.1s 82 Step: 21.322%, Train loss: 5.008, Val loss: 5.364, time: 0p50s
873.9s 83 Step: 42.644%, Train loss: 5.035, Val loss: 5.327, time: 0p50s
923.2s 84 Step: 63.966%, Train loss: 5.004, Val loss: 5.335, time: 0p49s
971.3s 85 Step: 85.288%, Train loss: 4.973, Val loss: 5.299, time: 0p48s
1005.1s 86 And I ' m going to be a lot of
1005.1s 87 Epoch: 4, Train loss: 4.956, Val loss: 5.284, Epoch time: 232.332s
1054.6s 88 Step: 21.322%, Train loss: 4.859, Val loss: 5.274, time: 0p49s
1104.9s 89 Step: 42.644%, Train loss: 4.889, Val loss: 5.292, time: 0p50s
1154.2s 90 Step: 63.966%, Train loss: 4.860, Val loss: 5.298, time: 0p49s
1202.2s 91 Step: 85.288%, Train loss: 4.829, Val loss: 5.209, time: 0p47s
1236.2s 92 And I ' m going to be a lot of
1236.2s 93 Epoch: 5, Train loss: 4.813, Val loss: 5.208, Epoch time: 231.070s
1285.6s 94 Step: 21.322%, Train loss: 4.728, Val loss: 5.225, time: 0p49s
1335.2s 95 Step: 42.644%, Train loss: 4.756, Val loss: 5.242, time: 0p49s
1383.8s 96 Step: 63.966%, Train loss: 4.732, Val loss: 5.226, time: 0p48s
1431.2s 97 Step: 85.288%, Train loss: 4.703, Val loss: 5.155, time: 0p47s
1464.7s 98 And it 's a lot of things .
1464.7s 99 Epoch: 6, Train loss: 4.686, Val loss: 5.155, Epoch time: 228.547s
1513.9s 100 Step: 21.322%, Train loss: 4.612, Val loss: 5.164, time: 0p49s
1563.9s 101 Step: 42.644%, Train loss: 4.642, Val loss: 5.158, time: 0p49s
1613.0s 102 Step: 63.966%, Train loss: 4.617, Val loss: 5.168, time: 0p49s
1660.5s 103 Step: 85.288%, Train loss: 4.590, Val loss: 5.099, time: 0p47s
1693.8s 104 And they 're going to be able to be .
1693.8s 105 Epoch: 7, Train loss: 4.573, Val loss: 5.109, Epoch time: 229.082s
1742.4s 106 Step: 21.322%, Train loss: 4.496, Val loss: 5.132, time: 0p48s
1791.6s 107 Step: 42.644%, Train loss: 4.529, Val loss: 5.098, time: 0p49s
1839.9s 108 Step: 63.966%, Train loss: 4.506, Val loss: 5.137, time: 0p48s
1886.8s 109 Step: 85.288%, Train loss: 4.480, Val loss: 5.017, time: 0p46s
1919.8s 110 And we 're going to be able to be able
1919.8s 111 Epoch: 8, Train loss: 4.464, Val loss: 5.053, Epoch time: 226.029s
1968.5s 112 Step: 21.322%, Train loss: 4.385, Val loss: 5.077, time: 0p48s
2017.7s 113 Step: 42.644%, Train loss: 4.420, Val loss: 5.051, time: 0p49s
2065.9s 114 Step: 63.966%, Train loss: 4.400, Val loss: 5.087, time: 0p48s
2113.4s 115 Step: 85.288%, Train loss: 4.380, Val loss: 4.977, time: 0p47s
2146.5s 116 We 're going to be able to be able to
2146.5s 117 Epoch: 9, Train loss: 4.364, Val loss: 5.001, Epoch time: 226.683s
2195.3s 118 Step: 21.322%, Train loss: 4.292, Val loss: 5.024, time: 0p48s
2244.8s 119 Step: 42.644%, Train loss: 4.326, Val loss: 5.006, time: 0p49s
2293.1s 120 Step: 63.966%, Train loss: 4.308, Val loss: 5.036, time: 0p48s
2340.3s 121 Step: 85.288%, Train loss: 4.286, Val loss: 4.938, time: 0p47s
2373.7s 122 We 're going to do .
2373.7s 123 Epoch: 10, Train loss: 4.272, Val loss: 4.990, Epoch time: 227.151s
2422.5s 124 Step: 21.322%, Train loss: 4.198, Val loss: 5.013, time: 0p48s
2471.9s 125 Step: 42.644%, Train loss: 4.236, Val loss: 4.973, time: 0p49s
2520.9s 126 Step: 63.966%, Train loss: 4.222, Val loss: 5.005, time: 0p49s
2568.9s 127 Step: 85.288%, Train loss: 4.205, Val loss: 4.937, time: 0p47s
2602.7s 128 We 're going to be able to be able to
2602.7s 129 Epoch: 11, Train loss: 4.191, Val loss: 4.992, Epoch time: 229.059s
2652.0s 130 Step: 21.322%, Train loss: 4.121, Val loss: 4.979, time: 0p49s
2701.8s 131 Step: 42.644%, Train loss: 4.160, Val loss: 4.951, time: 0p49s
2750.6s 132 Step: 63.966%, Train loss: 4.145, Val loss: 4.997, time: 0p48s
2798.0s 133 Step: 85.288%, Train loss: 4.125, Val loss: 4.919, time: 0p47s
2831.5s 134 We 're going to be able to do .
2831.5s 135 Epoch: 12, Train loss: 4.109, Val loss: 4.943, Epoch time: 228.751s
2880.4s 136 Step: 21.322%, Train loss: 4.033, Val loss: 4.940, time: 0p48s
2930.1s 137 Step: 42.644%, Train loss: 4.073, Val loss: 4.901, time: 0p49s
2978.6s 138 Step: 63.966%, Train loss: 4.056, Val loss: 4.981, time: 0p48s
3025.7s 139 Step: 85.288%, Train loss: 4.035, Val loss: 4.904, time: 0p47s
3058.9s 140 We 're going to be able to be able to
3058.9s 141 Epoch: 13, Train loss: 4.020, Val loss: 4.928, Epoch time: 227.419s
3107.5s 142 Step: 21.322%, Train loss: 3.946, Val loss: 4.934, time: 0p48s
3157.0s 143 Step: 42.644%, Train loss: 3.989, Val loss: 4.879, time: 0p49s
3205.9s 144 Step: 63.966%, Train loss: 3.971, Val loss: 5.012, time: 0p48s
3253.9s 145 Step: 85.288%, Train loss: 3.951, Val loss: 4.894, time: 0p47s
3287.8s 146 We 're all the world .
3287.8s 147 Epoch: 14, Train loss: 3.936, Val loss: 4.911, Epoch time: 228.872s
3337.1s 148 Step: 21.322%, Train loss: 3.869, Val loss: 4.917, time: 0p49s
3386.8s 149 Step: 42.644%, Train loss: 3.908, Val loss: 4.883, time: 0p49s
3435.3s 150 Step: 63.966%, Train loss: 3.889, Val loss: 4.989, time: 0p48s
3483.3s 151 Step: 85.288%, Train loss: 3.870, Val loss: 4.899, time: 0p47s
3516.8s 152 We 're all the way .
3516.8s 153 Epoch: 15, Train loss: 3.856, Val loss: 4.890, Epoch time: 228.985s
3565.7s 154 Step: 21.322%, Train loss: 3.787, Val loss: 4.897, time: 0p48s
3615.0s 155 Step: 42.644%, Train loss: 3.825, Val loss: 4.880, time: 0p49s
3663.4s 156 Step: 63.966%, Train loss: 3.805, Val loss: 4.957, time: 0p48s
3710.7s 157 Step: 85.288%, Train loss: 3.785, Val loss: 4.911, time: 0p47s
3744.0s 158 We 're all doing this .
3744.0s 159 Epoch: 16, Train loss: 3.770, Val loss: 4.836, Epoch time: 227.211s
3792.8s 160 Step: 21.322%, Train loss: 3.703, Val loss: 4.906, time: 0p48s
3842.2s 161 Step: 42.644%, Train loss: 3.742, Val loss: 4.901, time: 0p49s
3890.7s 162 Step: 63.966%, Train loss: 3.723, Val loss: 4.938, time: 0p48s
3938.4s 163 Step: 85.288%, Train loss: 3.702, Val loss: 4.912, time: 0p47s
3972.0s 164 We 're all all .
3972.0s 165 Epoch: 17, Train loss: 3.687, Val loss: 4.850, Epoch time: 228.002s
4021.7s 166 Step: 21.322%, Train loss: 3.618, Val loss: 4.928, time: 0p49s
4071.9s 167 Step: 42.644%, Train loss: 3.655, Val loss: 4.890, time: 0p50s
4120.8s 168 Step: 63.966%, Train loss: 3.635, Val loss: 4.923, time: 0p48s
4168.2s 169 Step: 85.288%, Train loss: 3.614, Val loss: 4.909, time: 0p47s
4201.7s 170 We 're all doing this .
4201.7s 171 Epoch: 18, Train loss: 3.598, Val loss: 4.816, Epoch time: 229.760s
4251.0s 172 Step: 21.322%, Train loss: 3.521, Val loss: 4.900, time: 0p49s
4300.9s 173 Step: 42.644%, Train loss: 3.560, Val loss: 4.845, time: 0p49s
4349.9s 174 Step: 63.966%, Train loss: 3.537, Val loss: 4.895, time: 0p48s
4397.4s 175 Step: 85.288%, Train loss: 3.518, Val loss: 4.833, time: 0p47s
4430.4s 176 We 're all here .
4430.4s 177 Epoch: 19, Train loss: 3.500, Val loss: 4.805, Epoch time: 228.647s
4479.1s 178 Step: 21.322%, Train loss: 3.421, Val loss: 4.911, time: 0p48s
4529.3s 179 Step: 42.644%, Train loss: 3.463, Val loss: 4.793, time: 0p50s
4577.9s 180 Step: 63.966%, Train loss: 3.436, Val loss: 4.885, time: 0p48s
4625.9s 181 Step: 85.288%, Train loss: 3.414, Val loss: 4.766, time: 0p48s
4659.8s 182 We 're all the way .
4659.8s 183 Epoch: 20, Train loss: 3.397, Val loss: 4.791, Epoch time: 229.450s
4709.6s 184 Step: 21.322%, Train loss: 3.316, Val loss: 4.857, time: 0p49s
4760.2s 185 Step: 42.644%, Train loss: 3.357, Val loss: 4.755, time: 0p50s
4809.7s 186 Step: 63.966%, Train loss: 3.329, Val loss: 4.829, time: 0p49s
4856.9s 187 Step: 85.288%, Train loss: 3.308, Val loss: 4.715, time: 0p47s
4890.3s 188 We 're all about .
4890.3s 189 Epoch: 21, Train loss: 3.289, Val loss: 4.767, Epoch time: 230.450s
4939.0s 190 Step: 21.322%, Train loss: 3.205, Val loss: 4.773, time: 0p48s
4989.2s 191 Step: 42.644%, Train loss: 3.239, Val loss: 4.715, time: 0p50s
5037.4s 192 Step: 63.966%, Train loss: 3.208, Val loss: 4.761, time: 0p48s
5085.0s 193 Step: 85.288%, Train loss: 3.186, Val loss: 4.650, time: 0p47s
5118.5s 194 We 're all about the people .
5118.5s 195 Epoch: 22, Train loss: 3.166, Val loss: 4.694, Epoch time: 228.279s
5168.3s 196 Step: 21.322%, Train loss: 3.082, Val loss: 4.722, time: 0p49s
5218.8s 197 Step: 42.644%, Train loss: 3.115, Val loss: 4.685, time: 0p50s
5268.2s 198 Step: 63.966%, Train loss: 3.086, Val loss: 4.733, time: 0p49s
5315.7s 199 Step: 85.288%, Train loss: 3.067, Val loss: 4.577, time: 0p47s
5349.2s 200 We 're all about are .
5349.2s 201 Epoch: 23, Train loss: 3.047, Val loss: 4.622, Epoch time: 230.623s
5398.5s 202 Step: 21.322%, Train loss: 2.962, Val loss: 4.640, time: 0p49s
5448.4s 203 Step: 42.644%, Train loss: 2.999, Val loss: 4.644, time: 0p49s
5497.3s 204 Step: 63.966%, Train loss: 2.968, Val loss: 4.709, time: 0p48s
5545.2s 205 Step: 85.288%, Train loss: 2.949, Val loss: 4.517, time: 0p47s
5578.5s 206 We 're the people who are are .
5578.5s 207 Epoch: 24, Train loss: 2.930, Val loss: 4.588, Epoch time: 229.349s
5627.4s 208 Step: 21.322%, Train loss: 2.851, Val loss: 4.572, time: 0p48s
5676.9s 209 Step: 42.644%, Train loss: 2.891, Val loss: 4.587, time: 0p49s
5725.4s 210 Step: 63.966%, Train loss: 2.857, Val loss: 4.635, time: 0p48s
5772.9s 211 Step: 85.288%, Train loss: 2.838, Val loss: 4.523, time: 0p47s
5806.5s 212 We 're the kids .
5806.5s 213 Epoch: 25, Train loss: 2.820, Val loss: 4.537, Epoch time: 227.955s
5855.4s 214 Step: 21.322%, Train loss: 2.731, Val loss: 4.526, time: 0p48s
5905.3s 215 Step: 42.644%, Train loss: 2.771, Val loss: 4.577, time: 0p49s
5954.0s 216 Step: 63.966%, Train loss: 2.739, Val loss: 4.584, time: 0p48s
6001.6s 217 Step: 85.288%, Train loss: 2.724, Val loss: 4.445, time: 0p47s
6035.0s 218 We 're the kids to each other .
6035.0s 219 Epoch: 26, Train loss: 2.706, Val loss: 4.512, Epoch time: 228.536s
6084.3s 220 Step: 21.322%, Train loss: 2.616, Val loss: 4.506, time: 0p49s
6134.0s 221 Step: 42.644%, Train loss: 2.659, Val loss: 4.647, time: 0p49s
6182.6s 222 Step: 63.966%, Train loss: 2.631, Val loss: 4.562, time: 0p48s
6230.2s 223 Step: 85.288%, Train loss: 2.622, Val loss: 4.435, time: 0p47s
6263.8s 224 We 're the kids to each other .
6263.8s 225 Epoch: 27, Train loss: 2.603, Val loss: 4.478, Epoch time: 228.768s
6313.0s 226 Step: 21.322%, Train loss: 2.503, Val loss: 4.525, time: 0p49s
6363.0s 227 Step: 42.644%, Train loss: 2.545, Val loss: 4.642, time: 0p49s
6411.9s 228 Step: 63.966%, Train loss: 2.521, Val loss: 4.512, time: 0p48s
6459.6s 229 Step: 85.288%, Train loss: 2.511, Val loss: 4.376, time: 0p47s
6493.2s 230 We 're the kids .
6493.2s 231 Epoch: 28, Train loss: 2.492, Val loss: 4.534, Epoch time: 229.461s
6543.3s 232 Step: 21.322%, Train loss: 2.406, Val loss: 4.638, time: 0p50s
6595.0s 233 Step: 42.644%, Train loss: 2.442, Val loss: 4.599, time: 0p51s
6645.1s 234 Step: 63.966%, Train loss: 2.415, Val loss: 4.556, time: 0p50s
6693.8s 235 Step: 85.288%, Train loss: 2.407, Val loss: 4.366, time: 0p48s
6728.0s 236 We 're the students to each other .
6728.0s 237 Epoch: 29, Train loss: 2.387, Val loss: 4.566, Epoch time: 234.761s
6778.2s 238 Step: 21.322%, Train loss: 2.308, Val loss: 4.616, time: 0p50s
6828.4s 239 Step: 42.644%, Train loss: 2.345, Val loss: 4.507, time: 0p50s
6877.6s 240 Step: 63.966%, Train loss: 2.314, Val loss: 4.536, time: 0p49s
6925.4s 241 Step: 85.288%, Train loss: 2.307, Val loss: 4.414, time: 0p47s
6959.2s 242 We are students - students .
6959.2s 243 Epoch: 30, Train loss: 2.286, Val loss: 4.563, Epoch time: 231.003s
6959.9s 244 We 're a young kids .
6964.7s 245 /opt/conda/lib/python3.10/site-packages/traitlets/traitlets.py:2930: FutureWarning: --Exporter.preprocessors=["remove_papermill_header.RemovePapermillHeader"] for containers is deprecated in traitlets 5.0. You can pass `--Exporter.preprocessors item` ... multiple times to add items to a list.
6964.7s 246 warn(
6964.7s 247 [NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
6964.7s 248 [NbConvertApp] Converting notebook __notebook__.ipynb to notebook
6965.3s 249 [NbConvertApp] Writing 52669 bytes to __notebook__.ipynb
6967.0s 250 /opt/conda/lib/python3.10/site-packages/traitlets/traitlets.py:2930: FutureWarning: --Exporter.preprocessors=["nbconvert.preprocessors.ExtractOutputPreprocessor"] for containers is deprecated in traitlets 5.0. You can pass `--Exporter.preprocessors item` ... multiple times to add items to a list.
6967.0s 251 warn(
6967.0s 252 [NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
6967.1s 253 [NbConvertApp] Converting notebook __notebook__.ipynb to html
6968.2s 254 [NbConvertApp] Writing 377058 bytes to __results__.html